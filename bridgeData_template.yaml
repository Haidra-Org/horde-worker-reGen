## Common for all worker types

# !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!
# See also the readme's "Suggested settings" section for recommended settings.    !!!
# !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!  !!!

# The Horde URL. Do not change this unless you are using a custom Horde.
horde_url: "https://aihorde.net/api/"

# The API key identifies a unique user in the Horde
# Visit https://aihorde.net/register to create one before you can join
api_key: "0000000000"

# List of usernames whose prompts you want to prioritize.
# The owner's username is always included, so you don't need to add it here if you use the key specified in `api_key` for requests.
priority_usernames: []

# The maximum number of parallel jobs to run at the same time.
# Only high-end cards (e.g., 3080 or better) benefit from this setting.
# If you have a 20xx or earlier, or a xx60/xx70, do not change this setting from 1.
max_threads: 1
# 24GB+ VRAM: 1 (2 max if Flux/Cascade loaded)
# 12GB-16GB VRAM: 1
# 8GB-10GB VRAM: 1

# Number of requests to keep in the queue to start working as soon as a thread is available.
# Generally should be 1 or 2. Never set this higher than 2 if your max_threads is 2.
# Warning: Increasing this value directly increases system RAM usage significantly.
queue_size: 1
# 24GB+ VRAM: 2 (3 if 64GB+ RAM)
# 8GB-10GB VRAM: 1 (max or only offer flux)

# Number of jobs to pull per request and perform batched inference.
# More optimized than doing them one by one but slower.
# Ensure you can generate your max_batch at half your max_power.
max_batch: 1
# 24GB+ VRAM: 8 or higher
# 12GB-16GB VRAM: 4 or higher
# 8GB-10GB VRAM: 4 (no higher than 4)

# Run CLIP model (checking for potential CSAM or NSFW) on GPU instead of CPU.
# Enable this on cards with 12GB or more VRAM to increase job completion rate.
# ~1.2GB of VRAM overhead
safety_on_gpu: false
# 24GB+ VRAM: true
# 12GB-16GB VRAM: true (consider false if offering Cascade or Flux)
# 8GB-10GB VRAM: false

# Only pick up jobs where the user has the required kudos upfront.
# Excludes all anonymous accounts and registered accounts who haven't contributed.
require_upfront_kudos: false

# If set, this worker will use this civitai API token when downloading any resources from civitai.
# This is required in order to provide LoRas/TIs (or other resources)
# which are marked as requiring a civitai token to download.
#
# If you set "models_to_load" to something like "top 5", it is necessary to provide this token,
# as many of the most popular models require it for the download. Downloading many models in bulk
# could also lead to issues if no token is required.
#
# You can get your civitai API Key from https://civitai.com/user/account (look for 'Add API Key')
#
# Remove the # from the line below and add your civitai API token to enable this feature.
# civitai_api_token:

#######################################
## Dreamer (Stable Diffusion Worker) ##
#######################################

# Worker name for running a Dreamer instance.
dreamer_name: "An Awesome Dreamer"

# Max resolution (max pixels) supported.
# Formula: `64 * 64 * 8 * max_power` (total pixels)
# Examples:
# 8  = 512x512
# 18 = 768x768
# 32 = 1024x1024
# 50 = 1280x1280
max_power: 8
# Suggested values:
#   8GB-10GB VRAM: 32 (no higher than 32)
#   12GB-16GB VRAM: 32-64 (no higher than 64)
#   24GB+ VRAM: 64-128 (no higher than 128)

# Use more VRAM on average but reduce time spent loading models.
high_memory_mode: false
# Suggested values:
#   24GB+ VRAM: true
#   12GB-16GB VRAM: true (consider false if offering Cascade or Flux)

# Fill local queue much faster but may be penalized by the server if you cannot keep up with jobs.
high_performance_mode: false
# Suggested values:
#   24GB+ VRAM: true

# Fill local queue somewhat faster but may be penalized by the server if you cannot keep up with jobs.
# Overridden by high_performance_mode.
moderate_performance_mode: false
# Suggested values:
#   12GB-16GB VRAM: true

# Start processing the next job before the current job finishes post-processing.
# Reduces time between jobs but may cause crashes on low RAM or VRAM systems.
post_process_job_overlap: false
# Suggested values:
#   24GB+ VRAM: true

# Aggressively unload models from VRAM when not in use.
# Should be true for most workers with GPUs with less than 16GB of VRAM.
unload_models_from_vram_often: true
# Suggested values:
#   24GB+ VRAM: false
#   12GB-16GB VRAM: false
#   8GB-10GB VRAM: true

# Normally only one model will load off disk at a time. Set to true to load multiple models at once.
# This requires a very fast disk. You will see a sharp increase in disk usage, especially with SDXL/Cascade/Flux/other large models.
very_fast_disk_mode: false

# List of words to reject if they appear in the prompt.
blacklist: []

# Serve NSFW images if true.
nsfw: true

# Censor NSFW images if true.
censor_nsfw: false

# List of words to always censor, even if `nsfw` is true.
censorlist: []

# Accept jobs using a user-supplied image.
allow_img2img: true

# Accept jobs using a user-supplied image and an inpainting-specific model.
# Forced to false if `allow_img2img` is false.
allow_painting: true

# Allow user requests from behind VPNs.
# Note: The worker does not directly interact with user IPs - it only interacts with the StableHorde API.
allow_unsafe_ip: true

# Allow upscaling, facefixer, and other post-generation features.
allow_post_processing: true
# 8GB-10GB VRAM: false (if offering SDXL or Flux, otherwise true)

# Allow ControlNet jobs.
# Note: Additional RAM/VRAM overhead. Low VRAM cards (<6GB) should be cautious.
allow_controlnet: false

# Allow SDXL jobs with high memory add-ons like ControlNet or transparency.
# Note: Significant additional RAM/VRAM overhead. Medium VRAM cards (<12GB) should be cautious.
# Note that if this is true, allow_controlnet must also be true.
allow_sdxl_controlnet: false
# 16GB+ VRAM: true
# 8GB-10GB VRAM: false

# Allow LoRas to be used. Requires a fast internet connection.
# LoRas will be downloaded on demand. `max_lora_cache_size` controls how many gigabytes to keep downloaded.
# 5GB of preselected LoRas are always downloaded the first time you start the worker with this setting.
# Note that there can be a significant delay when downloading LoRas causing GPU downtime.
allow_lora: false

# Delete any unknown LoRas from the loras folder when `download_models.py` is run.
# Warning: This option will delete any LoRas not in the model reference, including custom LoRas.
purge_loras_on_download: false

# Number of gigabytes of LoRas to keep cached. Minimum is 10GB.
max_lora_cache_size: 10

# Set to true if your worker is extraordinarily slow (below 0.1 mps/s).
# Users can choose to skip it when requesting generations, but job timeout and request expiry timeout are tripled.
extra_slow_worker: false
# Low-end cards or low performance: true

# Only pick up jobs requesting steps lower than the model's average steps.
# Useful for slower workers or if you don't want to serve requests with an extraordinary number of steps.
limit_max_steps: false
# Low-end cards or low performance: true

# Automatically determine the models with the highest queue and offer those.
dynamic_models: false # Currently unused in reGen

# Number of models to offer when `dynamic_models` is true.
number_of_dynamic_models: 0 # Currently unused in reGen

# Maximum number of models to download automatically for `dynamic_models`.
max_models_to_download: 10 # Currently unused in reGen

# Frequency (in seconds) to output worker summary stats, such as kudos per hour.
# Set to zero to disable stats output completely.
stats_output_frequency: 30

# Location where models are stored.
cache_home: "./models/"

# Location of the temp directory, also used for the model cache.
temp_dir: "./tmp" # Currently unused in reGen

# Always download models when required without prompting.
always_download: true # Currently unused in reGen

# Disable the terminal GUI, which displays information about the worker and the Horde.
disable_terminal_ui: false # Currently unused in reGen

# Obsolete
vram_to_leave_free: "80%" # Currently unused in reGen

# Target amount of system RAM to keep free.
# The worker only makes a best effort. Avoid using too much RAM with other programs.
ram_to_leave_free: "80%" # Currently unused in reGen

# Obsolete
disable_disk_cache: false # Currently unused in reGen

# Models to use.
# Instead of a model name, you may use any of the following magic constants:
#   "ALL" - Load all possible models (over 1TB of space).
#   "TOP n" - Load the top "N" most popular models (e.g., "top 5").
#   "BOTTOM n" - Load the bottom "N" models (e.g., "bottom 5").
#   "ALL SD15 MODELS" - All Stable Diffusion 1.5 models.
#   "ALL SD21 MODELS" - All Stable Diffusion 2.0/2.1 models.
#   "ALL SDXL MODELS" - All Stable Diffusion XL models.
#   "ALL INPAINTING MODELS" - All models marked for inpainting.
#   "ALL SFW MODELS" - All models marked as SFW.
#   "ALL NSFW MODELS" - All models marked as NSFW.
# The official model reference (in JSON format) is at https://github.com/Haidra-Org/AI-Horde-image-model-reference/blob/main/stable_diffusion.json.
# The model name must match the name in the model reference or be a magic constant.
# If you use `TOP` or `ALL` load commands, you should set a `civitai_api_token`. See that configuration entry for more information.
models_to_load:
  - "top 2"
  #- "ALL MODELS"
  #- "TOP 3"
  #- "ALL SFW"
  #- "Flux.1-Schnell fp8 (Compact)"
  #- "stable_diffusion"
  #- "Anything Diffusion"
  #- "stable_diffusion_inpainting"

# If you use a meta command, such as ALL or TOP n, you can allow very large models, such as cascade or flux to be included.
# By default, these models are excluded due to their large size.
# Set to true if have a 24GB card and want to include these models.
# Otherwise, I suggest including the models you know you can handle manually.
load_large_models: false

# Models to skip when `dynamic_models` is true or TOP n models are selected in models_to_load.
# Avoid loading models due to VRAM constraints, NSFW content, or other reasons.
models_to_skip:
  - "pix2pix" # Not currently supported
  - "SDXL_beta::stability.ai#6901" # Do not remove this, as this model would never work
  - "A to Zovya RPG" # This model is known to cause problems with reGen
  # - "Stable Cascade 1.0" # This a *very* VRAM intensive model
  # - ALL NSFW MODELS
  #- "stable_diffusion_inpainting"  # Inpainting is generally quite heavy along with other models for smaller GPUs.
  #- "stable_diffusion_2.1"  # Stable diffusion 2.1 has bigger memory requirements than 1.5, so if your card cannot lift, it, disable it
  #- "stable_diffusion_2.0"  # Same as Stable diffusion 2.1
  # - HASDX # Un-pruned SD1.5 model (5gb+)
  # - Anygen # Un-pruned SD1.5 model (5gb+)
  # - PFG # Un-pruned SD1.5 model (5gb+)
  # - Poison # Un-pruned SD1.5 model (5gb+)
  # - MoistMix # Un-pruned SD1.5 model (5gb+)
  # - Laolei New Berry Protogen Mix # Un-pruned SD1.5 model (5gb+)

# Suppress speed warnings if jobs are taking too long.
# Note: If you are getting these messages, you are serving jobs much slower than ideal.
# Lower your max_power for more kudos/hr.
suppress_speed_warnings: false

# Exit if an unhandled fault occurs. Useful for setting up the worker as a system service.
exit_on_unhandled_faults: false

#########################
## Scribe (LLM Worker) ##
#########################

# Worker name for running a Scribe worker.
scribe_name: "An Awesome Scribe"

# KoboldAI Client API URL.
kai_url: "http://localhost:5000"

# Max tokens to generate with this worker.
max_length: 80

# Max tokens to use from the prompt.
max_context_length: 1024

# Append the Horde alias behind the API key to the model advertised to the Horde.
# Prevents the model from being used from the shared pool but ensures no other worker can pretend to serve it.
branded_model: true

## Alchemist (Image Interrogation and Post-Processing)

# Worker name for running an Alchemist worker.
alchemist_name: "An Awesome Alchemist"

# Alchemy forms this worker can serve.
forms:
  - "caption"
  - "nsfw" # Uses CPU
  # Heavier than the others, but rewards more kudos
  - "interrogation"
  - "post-process"
